---
## Variables to be used for an automated NSX Deployments.

## Deployment methods
## Playbooks will default to answerfile.yml
## Example: ansible-playbook deploy-nsx.yml
## You can "EXTRA_VARS" files to overwrite the answerfile.yml
## Example: ansible-playbook -e @answerfile-lab1.yml -e nsx_license="XXXXX-XXXXX-XXXXX-XXXXX-XXXXX" deploy-nsx.yml  (Add different license)
## Note you MUST add the @ in front of a filename
## You can add --check or -C at the end of "ansible-playbook" line to do a dry-run. (Will create false error for playbook 10)
## Note (dynamic) in an area means this you can repeat the same code for multiple actions (e.g. Multiple Compute Managers, Uplink Profiles, Edge Nodes Deployments etc)
## If you're in a (dynamic) playbook and only have a single object to add but two are in the example, remove the extra lines or add ## in fron of the lines you do not want to be processed -
## (E.g. Compute Managers Section Everything under ## Start of 2nd Compute Manager would have a ## in front, so that only a single Compute Manager would be added)
## If you do NOT want a section to run ## out the - import_playbook line in the deploy YAML file. (deploy-nsx.yml)
## Confirmed working for 3.1.3.7 and 3.2.0.1 and 4.0.1
## Make note of how some varibles are supplied objects with a - front of the varible is in list form, some will be nested under the -.  Some objects can be change from a list object to dictonary object
## if changing from multiple objects to single object.  /examples/answerfile-example.yml can be used for reference

## Ansibe Variables
## Global State can be present (build) or absent (remove) (Used for all playbooks)
state: present                                                            ## present = create / absent = remove

## NSX Manager OVA Variables 
## Extra Deployment requirements:
  ##    - PyVmOmi - Python library for vCenter api.                       ## Sections will have URL to backend python file - Used to understand calls
  ##    - OVF Tools >= 4.3 - Ovftool is used for ovf deployment.          ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_deploy_ova.py
## Used in EVERY playbooks (Called to connect and run playbooks)          ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_virtual_ip.py
## Also used for Deployment / Configuration of playbooks 01-03            ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_licenses.py
## 
ovftool_path: "/usr/bin"                                                  ## Location of "ovftool" [ls /usr/bin/ovf* - to confirm location]
nsx_ova_path: "/tmp"                                                      ## Location of NSX Manager OVA File
nsx_ova_file: "nsx-embedded-unified-appliance-4.0.0.1.0.20159694.ova"     ## NSX Manager OVA File Name
nsx_size: "small"                                                         ## Size of NSX Manager (extrasmall (not really used) / small / medium / large) [must be lower case]
nsx_virtual_machine_name: "NSX-Manager-01"                                ## How the VM will be displayed in vCenter
nsx_hostname: "NSX-Manager-01"                                            ## This hostname will be used for deployment and status checks
nsx_domain: 'vmware.lab'                                                  ## Domain will be added to nsx_hostname for deployment to give FQDN calls
nsx_deployment_vcenter: "10.255.10.100"                                   ## Deploys NSX Manger OVA to this vCenter (IP or FQDN)
nsx_deployment_vcenter_username: "administrator@vsphere.local"            ## User used to connect to vCenter
nsx_deployment_vcenter_password: "VMware$123"                             ## Password used to connect to vCenter
nsx_datacenter_name: "Datacenter"                                         ## Datacenter Name - Must exist in above selected vCenter (nsx_deployment_vcenter)
nsx_vcenter_cluster: "Cluster1"                                           ## Cluster Name - Must exist in above selected vCenter (nsx_deployment_vcenter)
nsx_datastore: "Datastore-01"                                             ## Datastore Name - Must exist in above selected vCenter (nsx_deployment_vcenter)
nsx_port_group: "Management-Portgroup"                                    ## Portgroup Name - Must exist in above selected vCenter (nsx_deployment_vcenter)
nsx_ip_address: "10.255.11.157"                                           ## IP address of NSX Manager being deployed
nsx_netmask: "255.255.255.0"                                              ## Netmask of IP address of NSX Manager being deployed
nsx_gateway: "10.255.11.1"                                                ## Gateway IP address for NSX Manager being deployed
nsx_dns_server: "10.255.10.200"                                           ## DNS server for NSX Manager being deployed
nsx_ntp_server: "10.255.11.199"                                           ## NTP server for NSX Manager being deployed
nsx_vip: "10.255.11.156"                                                  ## Virtual IP address will be assigned to the NSX Manager Cluster
nsx_role: "NSX Manager"                                                   ## Needed role for NSX Manager deployment [NSX Global Manager or NSX Manager or nsx-cloud-service-manager]
nsx_admin_user: "admin"                                                   ## This user will be used for deployment and status checks
nsx_admin_password: "NSX$321nsx$321"                                      ## This password will be used for deployment and status checks
nsx_cli_password: "NSX$321nsx$321"                                        ## CLI Password - This is the root password
nsx_license: "XXXXX-XXXXX-XXXXX-XXXXX-XXXXX"                              ## Note that you can change on a single run with the -e command (e.g. ansible-playbook ... -e nsx_license="XXXXX-XXXXX-XXXXX-XXXXX-XXXXX")
nsx_ssh_enabled: true                                                     ## Enable SSH Access
nsx_ssh_root_login: true                                                  ## Allow Root SSH Access
nsx_validate_certs: False                                                 ## Checks certs during deployment

## Compute Managers Section (dynamic)
## Used for playbook 04                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_fabric_compute_managers.py
compute_managers:                                                         ## Each new Compute Manager will start with - display_name
- display_name: "nsx-vcenter"                                             ## Display Name of this new Compute Manager (Name of how you see this Compute Manager)
  server: "nsx-vcenter.vmware.lab"                                        ## This is the IP / FQDN of the new Compute Manager
  oidc_provider: False                                                    ## False or True (False Defualt) - "Enable Trust" Specifies whether compute manager has been set as OIDC provider If the compute manager is VC and need to set set as OIDC provider for NSX then this flag should be set as true. This is specific to TKGS. NSX-T 3.0 only
  credentials:
    credential_type: UsernamePasswordLoginCredential                      ## Possible values are 'UsernamePasswordLoginCredential', 'VerifiableAsymmetricLoginCredential', 'SessionLoginCredential'
    username: "administrator@vsphere.local"                               ## User to use to connect to this new Compute Manager
    password: "VMware1!"                                                  ## Password for user used to connect to this Compute Manager
## Start of 2nd Compute Manager
- display_name: "dev-vcenter"
  server: "dev-vcenter.vmware.lab"
  credentials:
    credential_type: UsernamePasswordLoginCredential
    username: "administrator@vsphere.local"
    password: "VMware$123"

## Uplink Profiles Section (dynamic)
## Used for playbook 05                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_uplink_profiles.py
uplink_profiles:                                                          ## Each new Uplink Profile will start with - display_name
- display_name: "UP-ESXi-MTEP"                                            ## Display Name of this new Uplink Profile
  description: "ESXi Host Multi-TEP Uplink Profile"                       ## Description of Uplink Profile (Not Required)
  #mtu: "1700"                                                            ## MTU (1280-9000) Note: MTU is not applicable for VDS. (Use Global MTU) (Not Required)
  teaming:                                                                ## Teamings section of Uplink Profiles - [Default Teaming] (active_list / standby_list [if needed])
    active_list:                                                          ## Active Uplinks section of Uplink Profiles
    - uplink_name: "vmnic0"                                               ## Name under Active Uplinks
      uplink_type: PNIC                                                   ## PNIC or LAG
    - uplink_name: "vmnic1"                                               ## If adding a 2nd NIC for Multiple TEPs
      uplink_type: PNIC                                                   ## PNIC or LAG
    policy: LOADBALANCE_SRCID                                             ## Teaming Policy (LOADBALANCE_SRCRD / FAILOVER_ORDER / LOADBALANCE_SRC_MAC)
  transport_vlan: 140                                                     ## Transport VLAN (VLAN TEPs will use)
  named_teamings:                                                         ## Optional - Named Teamings - Teamings outside of [Default Teaming] (active_list / standby_list [if needed])
  - name: "TOR-1"                                                         ## Optional - Name for this Teaming
    active_list:                                                          ## Optional - Active Uplinks section of Uplink Profiles
      - uplink_name: "vmnic0"                                             ## Optional - Name under Active Uplinks
        uplink_type: PNIC                                                 ## Optional - PNIC or LAG
    policy: FAILOVER_ORDER                                                ## Optional - Teaming Policy (LOADBALANCE_SRCRD / FAILOVER_ORDER / LOADBALANCE_SRC_MAC)
  - name: "TOR-2"                                                         ## Optional - 2nd Named Teaming Policy for this Teaming (Follows like the first)
    active_list:                                                            
      - uplink_name: "vmnic1"                                               
        uplink_type: PNIC                                                   
    policy: FAILOVER_ORDER                                                  
## Start of 2nd Uplink Profile 
- display_name: "UP-EDGE-MTEP"
  description: "NSX Edge Multi-TEP Uplink Profile"
  #mtu: "1700"
  teaming:
    active_list:
    - uplink_name: "fp-eth0"
      uplink_type: PNIC
    - uplink_name: "fp-eth1"
      uplink_type: PNIC
    policy: LOADBALANCE_SRCID
  transport_vlan: 141
  named_teamings:
  - name: "TOR-1"
    active_list:
      - uplink_name: "fp-eth0"
        uplink_type: PNIC
    policy: FAILOVER_ORDER
  - name: "TOR-2"
    active_list:
      - uplink_name: "fp-eth1"
        uplink_type: PNIC
    policy: FAILOVER_ORDER  
## Start of 3rd Uplink Profile
- display_name: "UP-ESXi-STEP"
  description: "ESXi Host Single TEP Uplink Profile"
  #mtu: "1700"
  teaming:
    active_list:
    - uplink_name: "vmnic0"
      uplink_type: PNIC
    standby_list:
    - uplink_name: "vmnic1"
      uplink_type: PNIC
    policy: FAILOVER_ORDER
  transport_vlan: 140
## Start of 4th Uplink Profile
- display_name: "UP-EDGE-STEP"
  description: "NSX Edge Single TEP Uplink Profile"
  #mtu: "1700"
  teaming:
    active_list:
    - uplink_name: "fp-eth0"
      uplink_type: PNIC
    standby_list:
    - uplink_name: "fp-eth1"
      uplink_type: PNIC
    policy: FAILOVER_ORDER
  transport_vlan: 141

## IP Pools Section (dynamic)
## Used for playbook 06                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_ip_pools.py
ip_pools:                                                                 ## Each new IP Pool will start with - display_name (Answerfile / Playbook will follow standard StaticIpPoolSpec)
- display_name: "IP-ESXi-TEP-Pool"                                        ## Display Name of this new IP Pool 
  description: "ESXi TEP IP Pool - Created by Ansible"                    ## Optional - Description for IP Pool
  subnets:                                                                ## Setnets / IP section
  - allocation_ranges:                                                    ## IP Range Info
    - start: "172.16.140.10"                                              ## Start IP Address
      end: "172.16.140.30"                                                ## End IP Address
    cidr: "172.16.140.0/24"                                               ## IP CIDR [e.g. 10.10.10.0/24]
    gateway_ip: "172.16.140.1"                                            ## Gateway IP Address ## if not needed
    state: present                                                        ## State "present" needed to create Required
    id: "IP-ESXi-TEP-Pool"                                                ## id or display_name needed under allocation_ranges
  
## 2nd IP Pool
- display_name: "IP-EDGE-TEP-Pool"
  description: "Edge TEP IP Pool - Created by Ansible"
  subnets:
  - allocation_ranges:
    - start: "172.16.141.10"
      end: "172.16.141.30"
    cidr: "172.16.141.0/24"
    gateway_ip: "172.16.141.1"
    state: present
    display_name: "IP-EDGE-TEP-Pool"
  

## Transport Zone Section (dynamic)
## Note - For 3.1 (3.2 is not a problem) using the default transport zone names of [nsx-overlay-transportzone and nsx-vlan-transportzone] will cause an error.  Use a unqiue name or remove the two default names.
## Used for playbook 07                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_transport_zones.py
transportzones:                                                           ## Each new Transport Zone will start with - display_name
- display_name: "nsx-overlay-transportzone"                               ## Display Name of this Transport Zone
  description: "Edge TEP IP Pool - Created by Ansible"
  transport_type: "OVERLAY"                                               ## Type of Transport Zone (OVERLAY or VLAN)
- display_name: "nsx-vlan-transportzone"
  description: "Ansible Created NSX VLAN Transportzone"
  transport_type: "VLAN"
  uplink_teaming_policy_names:                                            ## Name of the Named Teaming Policies defined in Uplink Profile Sections named_teamings/name (MUST BE type transport_type: VLAN)
  - "TOR-1"                                                               ## 1st Named Teaming Policy
  - "TOR-2"                                                               ## 2nd Named Teaming Policy

## Transport Node Profiles (dynamic)
## Should call already created objects
## Used for playbook 08                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_transport_node_profiles.py
transport_node_profiles:                                                  ## Each Transport Node Profile (TNP) will start with - display_name
- display_name: "TNP-Compute"                                             ## Display Name of Transport Node Profile
  description: "Ansible provisioned - Transport Node Profile  - Compute"  ## Optional - Description for this Transport Node Profile
  host_switches:                                                          ## Network configuration of this Transport Node Profile
  - host_switch_profiles:                                                 ## 
    - name: "UP-ESXi-MTEP"                                                ## Uplink Profile Name (Defined in Uplink Profile Section uplink_profiles/display_name)
      type: "UplinkHostSwitchProfile"                                     ## Type: UplinkHostSwitchProfile or LldpHostSwitchProfile
    host_switch_type: "VDS"                                               ## VDS or NVDS 
    host_switch_name: "vCenter7-VDS"                                      ## VDS Name 
    host_switch_mode: "STANDARD"                                          ## STANDARD or ENS (Enhanced Data Path) or ENS_INTERRUPT
    ## pnics - if using NVDS                                              ## Used if "host_switch_type" = NVDS - Comment pnic block out if using VDS
    # pnics:                                                              ## pnics defined for this Transport Node Profile using NVDS
    # - device_name: "vmnic2"                                             ## Physical Device on Host Transport Node
    #   uplink_name:  "vmnic2"                                            ## Uplink name defined in Uplink Profile Name
    # - device_name: "vmnic3"                                             ## 2nd Physical Device on Host Transport Node (if used)
    #   uplink_name:  "vmnic3"                                            ## 2nd Uplink name defined in Uplink Profile Name (if used)
    ## uplinks - if using VDS                                             ## Used if "host_switch_type" = VDS - Comment uplink block out if using NVDS
    uplinks:                                                              ## Uplinks defined for this Transport Node Profile using VDS
    - uplink_name: "vmnic0"                                               ## Follows the name given to Active Uplinks from selected Uplink Profile Name (Defined in Uplink Profile Section teaming/active_list/uplink_name)
      vds_uplink_name: "dvUplink1"                                        ## Follows the VDS Uplink name if backing by VDS
    - uplink_name: "vmnic1"                                               ## 2nd Follows the name given to Active Uplinks from selected Uplink Profile Name (Defined in Uplink Profile Section teaming/active_list/uplink_name) *If using Multiple TEPs
      vds_uplink_name: "dvUplink2"                                        ## 2nd Follows the VDS Uplink name if backing by VDS *If using Multiple TEPs
    ip_assignment_spec:                                                   ## IP Assignment (TEP) Section
      resource_type: "StaticIpPoolSpec"                                   ## StaticIpPoolSpec / StaticIpListSpec / AssignedByDhcp (Answerfile / Playbook will follow standard StaticIpPoolSpec)
      ip_pool_name: "IP-ESXi-TEP-Pool"                                    ## Name of Static IP Pool (Defined IP Pools Section ip_pools/display_name)
    transport_zone_endpoints:                                             ## Which Transport Zones are going to be connected to this Transport Node Profile (Defined in Transport Zone Section transportzones/display_name)
    - transport_zone_name: "nsx-overlay-transportzone"                    ## 1st connected Transport Zone Name
    - transport_zone_name: "nsx-vlan-transportzone"                       ## 2nd connected Transport Zone Name

## Configure Host Transport Nodes (dynamic)
## Unique display_name for each vCenter cluster - Cannot deploy to multiple clusters under one display_name
## This is a Manual Removal process (state of absent) will NOT remove, it will just "Detach Transport Node Profile"
## Used for playbook 09                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_transport_node_collections.py
host_transport_nodes:                                                     ## Each Host Transport Node will start with - display_name (Note The same TNP can be used over and over again, display_name MUST be unique for EACH cluster and only a single cluster per display_name)
- display_name: "TNC1"                                                    ## Display name (Not seen in UI)
  description: "Transport Node Collections 1"                             ## Optional - Description of this Transport Node Collection
  compute_manager_name: "nsx-vcenter"                                     ## Compute Manager Name (Defined in Compute Managers Section compute_manages/display_name)
  cluster_name: "Cluster-1"                                               ## Cluster Name - Must be a cluster inside the selected Compute Manager (Note only 1 cluster per display_name)
  transport_node_profile_name: "TNP-Compute"                              ## Name of TNP (Defined in Transport Node Profiles Section transport_node_profiles/display_name)

## Deploy Edge Nodes (dynamic)
## Should call already created objects
## Used for playbook 10                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_transport_nodes.py
## Start of 1st Edge Node Configuration
edge_transport_nodes:                                                     ## Each Edge Transport Node will start with - display_name (Meaning a new VM based Edge Transport Node will be deployed)
- display_name: "Edge-Node-01"                                            ## Display Name (How you see this Edge Transport Node in NSX and how the VM will be named in vCenter when deployed) 
  description: "Edge Transport Node 01- Deployed by Ansible"              ## Optional - Description for Edge Transport Node      
  host_switches:                                                          ## Network configuration of this Edge Transport Profile
  - host_switch_profiles:                                                 ##
    - name: "UP-EDGE-MTEP"                                                ## Uplink Profile Name (Defined in Uplink Profile Section uplink_profiles/display_name)
      type: UplinkHostSwitchProfile                                       ## Type: UplinkHostSwitchProfile or LldpHostSwitchProfile
    host_switch_mode: STANDARD                                            ## host_switch_mode With 3.2 is automaticly added, if deploying 3.1 it MUST be there must be inline with that start of "- name:..."
    host_switch_name: "nsxDefaultHostSwitch"                              ## Edge Transport Node internal "Edge Switch Name"
    pnics:                                                                ## PNICS will follow how your host_switch_profiles/name is configured.  If you have single you'll only have one PNICS/device_name and uplink/name
    - device_name: "fp-eth0"                                              ## Device name follow the datapath device that uplink_name will connect to within the Edge Transport Node (fp-eth0/1/2)
      uplink_name: "fp-eth0"                                              ## Follows the name of the Active Uplink in the selected Uplink Profile (host_switch_profile/name:)
    - device_name: "fp-eth1"                                              ## 2nd Device name follow the datapath device that uplink_name will connect to within the Edge Transport Node (fp-eth0/1/2)
      uplink_name: "fp-eth1"                                              ## 2nd Follows the name of the Active Uplink in the selected Uplink Profile (host_switch_profile/name:)    
    ip_assignment_spec:                                                   ## IP Assignment Spec (TEPs)
      resource_type: "StaticIpPoolSpec"                                   ## StaticIpPoolSpec / StaticIpListSpec / AssignedByDhcp (Answerfile / Playbook will follow standard StaticIpPoolSpec)
      ip_pool_name: "IP-EDGE-TEP-Pool"                                    ## Name of Static IP Pool (Defined IP Pools Section ip_pools/display_name)
    transport_zone_endpoints:                                             ## Which Transport Zones are going to be connected to this Edge Transport Node (Defined in Transport Zone Section transportzones/display_name)
      - transport_zone_name: "nsx-overlay-transportzone"                  ## 1st connected Transport Zone Name 
      - transport_zone_name: "nsx-vlan-transportzone"                     ## 2nd connected Transport Zone Name
  node_deployment_info:                                                   ## Edge Transport Node VM deployment info
    resource_type: "EdgeNode"                                             ## Type EdgeNode
    node_settings:                                                        ## VM deployment settings (Note - Watch the syntax here most objects are of "list" type, meaning they have a - in front of the object)
      hostname: "Edge-Node-01.vmware.lab"                                 ## Edge Transport Node hostname FQDN
      search_domains:                                                     ## DNS Search Domains
      - "vmware.lab"                                                      ## 1st DNS Search Name
      dns_servers:                                                        ## DNS Servers
      - "10.255.10.200"                                                   ## 1st DNS IP Address
      - "10.255.10.201"                                                   ## 2nd DNS IP Address
      ntp_servers:                                                        ## NTP Servers
      - "10.255.10.199"                                                   ## 1st NTP Server
      - "10.255.11.199"                                                   ## 2nd NTP Server
      allow_ssh_root_login: true                                          ## Allow Root Login over SSH (Should be disabled - Security yo!)
      enable_ssh: true                                                    ## Enable SSH Access (Should be disabled - Security yo!)
    deployment_config:                                                    ## Deployment Configuration Specs
      form_factor: "MEDIUM"                                               ## Form Factor of Edge Transport Node (SMALL / MEDIUM / LARGE / XLARGE) [must be UPPER case]
      node_user_settings:                                                 ## 
        cli_username: "admin"                                             ## CLI User name (Normally admin)
        cli_password: "NSX$321nsx$321"                                    ## CLI Password
        root_password: "NSX$321nsx$321"                                   ## Root password
      vm_deployment_config:                                               ## Edge Transport Node Deployment info
        placement_type: VsphereDeploymentConfig                           ## VsphereDeploymentConfig
        vc_name: "nsx-vcenter"                                            ## vCenter where Edge Node will be deployed  - Must be to a Compute Manager (Defined in Compute Managers Section compute_managers/display_name)
        vc_username: "administrator@vsphere.local"                        ## vCenter user name for connection (Not required but have had errors when not configured YMMV - Around deployment with reservation_info active)
        vc_password: "VMware$123"                                         ## vCenter user password (Not required but have had errors when not configured YMMV - Around deployment with reservation_info activeS)
        data_networks:                                                    ## Data_networks will tie your VDS Portgroup to your Uplinks defined in host_switch_profiles/name and PNICS area.  This is the "DPDK Fastpath Interfaces" section
        - "Edge-Trunk-A"                                                  ## VDS Portgroup name connecting to the 1st Uplink (e.g. fp-eth0)
        - "Edge-Trunk-B"                                                  ## VDS Portgroup name connecting to the 2nd Uplink (e.g. fp-eth1)
        management_network: "Management-Portgroup"                        ## VDS Portgroup name for eth0 (management)
        management_port_subnets:                                          ## Management Network IP info
        - ip_addresses:                                                   ## 
          - "10.255.11.171"                                               ## Management IP Address
          prefix_length: "24"                                             ## Management IP Address prefix 
        default_gateway_addresses:                                        ##
        - "10.255.11.1"                                                   ## Management Gateway IP Address
        compute: "Edge-Cluster"                                           ## vCenter Cluster Edge Transport Node will be deployed to
        storage: "Datastore-01"                                           ## vCenter Datastore Edge Transport Node will be deployed to
        reservation_info:                                                 ## Reservation Config Area
          memory_reservation:                                             ## Memory Reservation Area
            reservation_percentage: 0                                     ## Memory Reservation Percentage (100 is default [100%] however in lab deployments 0 can be used)

## Start of 2nd Edge Node Configuration ...

## Edge Cluster Configuration (dynamic)
## Should call already created objects
## Used for playbook 11                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_edge_clusters.py
edge_clusters:                                                            ## Each Edge Cluster will start with - display_name
- display_name: "Edge-Cluster-01"                                         ## Display name of Edge Cluster
  description: "Ansible provisionned - Edge Cluster"                      ## Optional - Description of Edge Cluster
  cluster_profile_bindings:                                               ## Edge Cluster Profile Info
  - profile_name: "nsx-default-edge-high-availability-profile"            ## Default nsx-default-edge-high-availability-profile
    resource_type: EdgeHighAvailabilityProfile                            ## EdgeHighAvailabilityProfile
  members:                                                                ## Which Edge Transport Nodes to add to Edge Cluster
    - transport_node_name: "Edge-Node-01"                                 ## 1st Edge Transport Node
    - transport_node_name: "Edge-Node-02"                                 ## 2nd Edge Transport Node

## Create Bridge Profiles (dynamic)
## Should call already created objects
## Used for playbook 12                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_policy_l2_bridge_ep_profile.py
edge_bridge_profile:                                                      ## Each Edge Bridge Profile will start with - display_name
- display_name: "Edge-Bridge-Profile-01"                                  ## Display name of Edge Bridge Profile
  description: "Edge Bridge Profile 01"                                   ## Optional - Description of Edge Bridge Profile
  edge_nodes_info:                                                        ## Required - Edge Bridge Profile Info
    - edge_cluster_display_name: "Bridge-Cluster-01"                      ## Required - Edge Cluster Display Name for this Edge Bridge Profile
      edge_node_display_name: "Bridge-Edge-Node-01"                       ## Required - Edge Node Name from Edge Cluster for Primary Node (There is NO supporting code for Backup Node) 
  failover_mode: NON_PREEMPTIVE                                           ## Optional - Failover mode NON_PREEMPTIVE or PREEMPTIVE (default)

## Create VLAN Backed Uplinks Segments for Edge Nodes (dynamic)
## Used for playbook 13                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_policy_segment.py
vlan_segments:                                                            ## Each VLAN Backed Segment will start with - display_name
- display_name: "Edge-Uplink-VLAN200"                                     ## Display name of VLAN Segment
  transport_zone_display_name: "nsx-vlan-transportzone"                   ## Display name of VLAN TransportZone
  vlan_ids: "200"                                                         ## Required - VLAN ID
  admin_state: "UP"                                                       ## Optional - Admin State of VLAN Segment (UP (default) / DOWN)
  advanced_config:                                                        ## Optional - Advanace Config
    connectivity: "ON"                                                    ## Optional - Connectivity (ON (default) / OFF) 
    uplink_teaming_policy_name: "TOR-1"                                   ## Optional - Connected to Uplink Teaming Policy
- display_name: "Edge-Uplink-VLAN201"                                      
  transport_zone_display_name: "nsx-vlan-transportzone"
  vlan_ids: "201"
  admin_state: "UP"
  advanced_config:
    connectivity: "ON"
    uplink_teaming_policy_name: "TOR-2"

## Create Tier0 Gateways (dynamic) WIP
## Should call already created objects
## This is a Manual Removal process (state of absent) will NOT remove currently
## Used for playbook 14                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_policy_tier0.py
tier0:                                                                    ## Each T0 Gateway will start with - display_name
- display_name: "Ansible-T0-Gateway"                                      ## Display name of T0 Gateway
  description: "Ansible T0 Gateway"                                       ## Optional - Description of T0 Gateway
  locale_services:                                                        ## T0 Locale Services 
  - display_name: "Ansible-T0-Gateway"                                    ## Display name for Locale Service (Would copy display_name)  
    do_wait_till_create: true                                             ## Optional - Can be used to wait for the realization of subresource
    #create_or_update_subresource_first: true                             ## Optional - Can be used to create subresources first
    state: "{{ state }}"                                                  ## State to create or remove objects
    route_redistribution_types:                                           ## Route Redistribution Types *See python file for types  
    - "TIER1_CONNECTED"                                                   ## Route Redistribution Type - selected
    route_redistribution_config:                                          ## Route Redistribution Config
      bgp_enabled: true                                                   ## Enable BGP
      redistribution_rules:                                               ## Route Redistribution Rules
      - name: "Route-Ridstribution"                                       ## Name of Route Redistribution Rule
        route_redistribution_types:                                       ## Route Redistribution Types *See python file for types
        - "TIER1_CONNECTED"                                               ## Route Redistribution Type - selected
    edge_cluster_info:                                                    ## Edge Cluster Info for T0 Gateway
      edge_cluster_display_name: "Edge-Cluster-01"                        ## Display name of Edge Cluster selected for T0 Gateway
    BGP:                                                                  ## BGP Configuration Area for T0 Gateway
        state: "{{ state }}"                                              ## State to create or remove objects
        local_as_num: "65310"                                             ## BGP AS Number for T0 Gateway
        neighbors:                                                        ## BGP Neighbors Area - Each different neighbor will start with - display_name
        - display_name: "Router-A"                                        ## BGP Name for 1st Neighbor (Required) Also not seen in UI 
          neighbor_address: "172.16.200.1"                                ## BGP Neighbor Address
          remote_as_num: "65200"                                          ## BGP Neighbor AS Number
          source_addresses:                                               ## BGP Source Addresses (Interfaces on T0 Gateway)
          - "172.16.200.20"                                               ## Interface IP addresses to talk to neighbor_address / Defined in Interfaces section 
          - "172.16.200.21"                                                 
          state: "{{ state }}"                                            ## State to create or remove objects
          bfd:                                                            ## BFD Area
            enabled: true                                                 ## BFD Enable for connection to neighbor_address
        - display_name: "Router-B"                                        
          neighbor_address: "172.16.201.1"
          remote_as_num: "65200"
          source_addresses:
          - "172.16.201.20"
          - "172.16.201.21"
          state: "{{ state }}"
          bfd:
            enabled: true
    interfaces:                                                           ## Interfaces configured on T0 Gateway - Each interface will start with - display_name
    - display_name: "Uplink-EN01-VLAN200"                                 ## Display name of interface
      do_wait_till_create: true                                           ## Optional - Can be used to wait for the realization of subresource
      state: "{{ state }}"                                                ## State to create or remove objects
      subnets:                                                            ## Interface configuration
      - ip_addresses:                                                     ## Interface IP Address Configuration
        - "172.16.200.20"                                                 ## IP Address of Interface
        prefix_len: "24"                                                  ## IP Address Prefix
      urpf_mode: "STRICT"                                                 ## uRPF (Unicast Reverse Path Forwarding) STRICT (default) or NONE *Must be all CAPS
      segment_display_name: "Edge-Uplink-VLAN200"                         ## Interface connected to Segment Name (Defined VLAN Segments)
      edge_node_info:                                                     ## Edge Node to Connection Info
        edge_cluster_display_name: "Edge-Cluster-01"                      ## Edge Cluster Edge Node is located in
        edge_node_display_name: "Edge-Node-01"                            ## Edge Node Interface is connected to
    - display_name: "Uplink-EN01-VLAN201"
      do_wait_till_create: true
      state: "{{ state }}"
      subnets:
      - ip_addresses:
        - "172.16.201.20"
        prefix_len: "24"
      urpf_mode: "STRICT"
      segment_display_name: "Edge-Uplink-VLAN201"
      edge_node_info:
        edge_cluster_display_name: "Edge-Cluster-01"
        edge_node_display_name: "Edge-Node-01"
    - display_name: "Uplink-EN02-VLAN200"
      do_wait_till_create: true
      state: "{{ state }}"
      subnets:
      - ip_addresses:
        - "172.16.200.21"
        prefix_len: "24"
      urpf_mode: "STRICT"
      segment_display_name: "Edge-Uplink-VLAN200"
      edge_node_info:
        edge_cluster_display_name: "Edge-Cluster-01"
        edge_node_display_name: "Edge-Node-02"
    - display_name: "Uplink-EN02-VLAN201"
      do_wait_till_create: true
      state: "{{ state }}"
      subnets:
      - ip_addresses:
        - "172.16.201.21"
        prefix_len: "24"
      urpf_mode: "STRICT"
      segment_display_name: "Edge-Uplink-VLAN201"
      edge_node_info:
        edge_cluster_display_name: "Edge-Cluster-01"
        edge_node_display_name: "Edge-Node-02"
        
## Create Tier1 Gateways (dynamic)
## Should call already created objects
## Used for playbook 15                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_policy_tier1.py
tier1:                                                                    ## Each T1 Gateway will start with - display_name
- display_name: "Ansible-Tier1-Gateway"                                   ## Display name of T1 Gateway
  description: "Ansible Tier1 Gateway"                                    ## Optional - Description of T1 Gateway
  tier0_display_name: "Ansible-T0-Gateway"                                ## Optional - Display name of T1 Gateway (Copy display_name from above)
  # locale_services:                                                      ## Optional - *See python file for usage (Services will need a name) - Example
  # - state: "{{ state }}"                                                ## Optional - *See python file for usage - Example
  #   display_name: "Ansible-T0-Gateway"                                  ## Optional - *See python file for usage - Example
  route_advertisement_types:                                              ## Optional - Route Adverisement Types
  - "TIER1_CONNECTED"                                                     ## Optional - Route Adverisement Type - *See python file for types
  - "TIER1_IPSEC_LOCAL_ENDPOINT"
- display_name: "TheOther-Ansible-Tier1-Gateway"                
  tier0_display_name: "Ansible-T0-Gateway"
  # locale_services:
  # - state: "{{ state }}"
  #   display_name: "Ansible-T0-Gateway"
  route_advertisement_types:
  - "TIER1_CONNECTED"
  - "TIER1_IPSEC_LOCAL_ENDPOINT"

## Create Segments (dynamic) (Overlay)
## (Connected or not connected to Gateways)
## May call already created objects
## Used for playbook 16                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_policy_segment.py                             
segments:                                                                 ## Each Segment will start with - display_name
- display_name: "Bridge-Segment-172.16.5.0-24"                            ## Display name of Segment
  description: "Ansible Bridge Segment"                                   ## Optional - Description of Segment
  transport_zone_display_name: "overlay-transportzone"                    ## Overlay Transport Zone Name
  connectivity_path: "/infra/tier-1s/Ansible-Tier1-Gateway"               ## Optional - Connectivity Path to T0 or T1 (Normally /infra/tier-#s/<Gateway Display Name>)
  subnets:                                                                ## Optional - Segment IP Info
  - gateway_address: "172.16.5.1/24"                                      ## Optional - Segment Gateway IP Address AND Prefix (x.x.x.x/x)
  admin_state: "UP"                                                       ## Optional - Admin State of Segment - UP (default) or DOWN
  advanced_config:                                                        ## Optional - Advance Config Info
    connectivity: "ON"                                                    ## Optional - Connectivity - ON (default) or OFF
  bridge_profiles:                                                        ## Optional - Connecting a Edge Bridge Profile to Segment
  - bridge_profile_path: "/infra/sites/default/enforcement-points/default/edge-bridge-profiles/Edge-Bridge-Profile-01"    ## Optional - Edge Bridge Profile Name to attach (You may need to confirm path w/ API calls)
    vlan_transport_zone_path: "/infra/sites/default/enforcement-points/default/transport-zones/5e7504e2-4701-44e1-80b6-6c3550846785"     ## Optional - VLAN Transport Zone - Have to use UUID (You may need to confirm path w/ API calls)
    vlan_ids: 0                                                           ## Optional - VLAN ID for Bridge (normally 0)
- display_name: "Web-Segment-172.16.6.0-24"                                      
  transport_zone_display_name: "overlay-transportzone"
  connectivity_path: "/infra/tier-1s/Ansible-Tier1-Gateway"
  subnets:
  - gateway_address: "172.16.5.1/24"

## Deploy Addional NSX Managers and Join to Cluster
## This playbook will call a connected Compute Manager
## This is a Manual Removal process (state of absent) will NOT remove
## Used for playbook 17                                                   ## https://github.com/vmware/ansible-for-nsxt/blob/master/plugins/modules/nsxt_manager_auto_deployment.py
addional_nsx_managers:                                                    ## Each new NSX Manager deployed to current NSX Manager Cluster starts with - nsx_size
## 2nd NSX Manager   
- nsx_size: "SMALL"                                                       ## Size of NSX Manager (EXTRASMALL / SMALL / MEDIUM / LARGE) [must be UPPER case]
  user_settings:                                                          ##
    cli_password: "NSX$321nsx$321"                                        ## CLI Password - This is the admin password    
    root_password: "NSX$321nsx$321"                                       ## Root Password
  deployment_config:                                                      ##
    placement_type: VsphereClusterNodeVMDeploymentConfig                  ## VsphereClusterNodeVMDeploymentConfig   
    vc_name: "vCenter"                                                    ## vCenter to deploy this NSX Manager (Must be a configured Compute Manager)
    vc_username: "administrator@vsphere.local"                            ## vCenter user
    vc_password: "VMware$123"                                             ## vCenter password
    hostname: "NSX-Manager-02.vmware.lab"                                 ## FQDN Host Name (Will be deployed to vCenter with short name)
    compute: "Cluster1"                                                   ## vCenter Cluster where NSX Manager will be deployed to
    storage: "Datastore-01"                                               ## vCenter Datastore where NSX Manager will be deployed to 
    management_network: "Management-Portgroup"                            ## Portgroup for Management Network  
    management_port_subnets:                                              ## Management Network IP info
    - ip_addresses:                                                       ##
      - "10.255.11.158"                                                   ## Management IP Address
      prefix_length: "24"                                                 ## Management IP Address Prefix
    default_gateway_addresses:                                            ##
    - "10.255.11.1"                                                       ## Management Gateway IP Address
    dns_servers:                                                          ## DNS Servers
    - "10.255.10.200"                                                     ## 1st DNS Server IP Address
    enable_ssh: true                                                      ## Enable SSH Access (Should be disabled - Security Yo!)
    allow_ssh_root_login: true                                            ## Allow Root Login over SSH (Should be disabled - Security Yo!)
## 3rd NSX Manager
- nsx_size: "SMALL"
  user_settings:
    cli_password: "NSX$321nsx$321"
    root_password: "NSX$321nsx$321"
  deployment_config:
    placement_type: VsphereClusterNodeVMDeploymentConfig
    vc_name: "vCenter"
    vc_username: "administrator@vsphere.local"
    vc_password: "VMware$123"
    hostname: "NSX-Manager-03.vmware.lab"
    compute: "Cluster1"
    storage: "Datastore-01"
    management_network: "Management-Portgroup"
    management_port_subnets:
    - ip_addresses: 
      - "10.255.11.159"
      prefix_length: "24"
    default_gateway_addresses:
    - "10.255.11.1"
    dns_servers:
    - "10.255.10.200"
    enable_ssh: true
    allow_ssh_root_login: true
...
